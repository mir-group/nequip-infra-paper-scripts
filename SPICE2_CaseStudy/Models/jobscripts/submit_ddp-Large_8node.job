#!/bin/bash
# SeÃ¡n Kavanagh, Marc Descoteaux 2024
#SBATCH -J Train
#SBATCH -p batch
#SBATCH -N 8
#SBATCH --ntasks-per-node=8 #Total number of tasks #### number of tasks per node (=number of GPUs per node), seems to be critical for ddp on Frontier!
#SBATCH --gres=gpu:8
#SBATCH --cpus-per-task=7  # number of CPUs per task, 56 allocatable CPUs per GPU (of which 8 per node) on Frontier by default
#SBATCH -t 0-02:00:00  # runtime in D-HH:MM, minimum of 10 minutes
#SBATCH -o runout.%j
#SBATCH -e runerr.%j
#SBATCH -A mat281
#SBATCH -C nvme



source /ccs/home/${USER}/.bashrc
conda activate /lustre/orion/mat281/proj-shared/25_01_09_SPICE-2_train/CondaEnv-10/allegro-restructure-torch26
module load craype-accel-amd-gfx90a
module load rocm/6.2.4

export MPICH_GPU_SUPPORT_ENABLED=1
module load libfabric/1.22.0
rocm-smi 
export LD_LIBRARY_PATH="${CRAY_LD_LIBRARY_PATH}:${LD_LIBRARY_PATH}"
export PL_RECONCILE_PROCESS=1

job_name=${SLURM_JOB_NAME}
run_name="SPICE-2_${job_name}"
infile=$1
outdir="./outputs/${infile}"

gpus_per_node=8  # on Frontier
nodes_per_train=8
gpus_per_train=$(( nodes_per_train * gpus_per_node ))
cores_per_node=56  # 64 cores per node on Frontier, but then one core reserved for each L3 region by default
cores_per_gpu=$(( cores_per_node / gpus_per_node ))  # Divide total cores by number of GPUs, should be 7 usually
NGPUS=$(( SLURM_NNODES * gpus_per_node ))  # 8 GPUs per node on Frontier
echo $(hostname -i)
export MASTER_ADDR=$(hostname -i)  # these are important when running multi-node jobs

export MASTER_PORT=3442
export NCCL_SOCKET_IFNAME=hsn0
export GLOO_SOCKET_IFNAME=hsn0

export NCCL_P2P_DISABLE=1

export NCCL_IB_DISABLE=1
export NCCL_NET_GDR_LEVEL=3
export NCCL_ALGO=Ring
export NCCL_CROSS_NIC=1

echo $(hostname -i)
export MASTER_ADDR=$(hostname -i)

export WANDB_MODE=offline

export NCCL_DEBUG=WARN

mkdir "/mnt/bb/${USER}/triton_cache"
export TRITON_CACHE_DIR="/mnt/bb/${USER}/triton_cache/"

mkdir "/mnt/bb/${USER}/kernel_cache"
export PYTORCH_KERNEL_CACHE_PATH="/mnt/bb/${USER}/kernel_cache"

echo "NNODES=${SLURM_NNODES}"
echo "NODE LIST=${SLURM_JOB_NODELIST}"
echo "NTASKS_PER_NODE=${SLURM_NTASKS_PER_NODE}"
echo "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}"  # should be empty on Frontier as it's AMD GPUs
echo "ROCR_VISIBLE_DEVICES=${ROCR_VISIBLE_DEVICES}"
echo "MASTER_ADDR=${MASTER_ADDR}"
job_ID=(${SLURM_JOBID//./ })

export OMP_NUM_THREADS=1  # faster processing with this
export HYDRA_FULL_ERROR=1  # full Hydra error traceback if any issues
export WORLD_SIZE=$SLURM_NNODES

infile=$1
outdir="./outputs/${infile}"
echo ${infile}
echo $infile
echo ${nodes_per_train}
echo ${cores_per_gpu}
echo ${gpus_per_node}
echo

export TORCH_NCCL_TRACE_BUFFER_SIZE=64
#Restart if the job exists, else start fresh
if [ -d "${outdir}" ];then
sbcast -pf ${outdir}/last.ckpt /mnt/bb/${USER}/last.ckpt
srun nequip-train -cn $infile  -cp "$(pwd)/configs/" hydra.verbose=true hydra.run.dir=$outdir ++ckpt_path="/mnt/bb/${USER}/last.ckpt" &>> output_${infile}.txt &
else
srun nequip-train -cn $infile  -cp "$(pwd)/configs/" hydra.verbose=true hydra.run.dir=$outdir &> output_${infile}.txt &
fi

wait; exit
